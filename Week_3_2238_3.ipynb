{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBTvvmlTzFlUmf6lghGN96",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saivamshi-2628/GEN-AI/blob/main/Week_3_2238_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the derivative f'(x)\n",
        "def df(x):\n",
        "    return 20 * x**3 + 6 * x\n",
        "\n",
        "# Gradient Descent implementation\n",
        "def gradient_descent(start_x, learning_rate, iterations):\n",
        "    x = start_x  # Initialize x\n",
        "    for i in range(iterations):\n",
        "        grad = df(x)  # Compute the gradient\n",
        "        x -= learning_rate * grad  # Update x\n",
        "        # Print progress for demonstration\n",
        "        print(f\"Iteration {i+1}: x = {x}, f(x) = {f(x)}\")\n",
        "    return x\n",
        "\n",
        "# Parameters\n",
        "start_x = 1.0       # Initial guess\n",
        "learning_rate = 0.01\n",
        "iterations = 10\n",
        "\n",
        "# Run Gradient Descent\n",
        "minimum_x = gradient_descent(start_x, learning_rate, iterations)\n",
        "\n",
        "# Print final result\n",
        "print(\"Value of x at minimum:\", minimum_x)\n",
        "print(\"Minimum value of f(x):\", f(minimum_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LtlDMjoK1Lb",
        "outputId": "d2451e1f-269e-46f4-cc79-ab49764d3371"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: x = 0.74, f(x) = 13.1421288\n",
            "Iteration 2: x = 0.6145552, f(x) = 11.846237994400788\n",
            "Iteration 3: x = 0.5312610807000426, f(x) = 11.245007398763406\n",
            "Iteration 4: x = 0.4693969671925482, f(x) = 10.903734822763694\n",
            "Iteration 5: x = 0.42054837262425754, f(x) = 10.686981750526822\n",
            "Iteration 6: x = 0.38043975469571134, f(x) = 10.538943463638885\n",
            "Iteration 7: x = 0.34660082495852806, f(x) = 10.43255504111426\n",
            "Iteration 8: x = 0.3174771962595419, f(x) = 10.35317021507909\n",
            "Iteration 9: x = 0.29202874676564666, f(x) = 10.292206431621567\n",
            "Iteration 10: x = 0.2695261335763863, f(x) = 10.244319008300756\n",
            "Value of x at minimum: 0.2695261335763863\n",
            "Minimum value of f(x): 10.244319008300756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Define the function g(x, y)\n",
        "def g(x, y):\n",
        "    return 3 * x**2 + 5 * math.exp(-y) + 10\n",
        "\n",
        "# Define the partial derivatives of g(x, y)\n",
        "def dg_dx(x, y):\n",
        "    return 6 * x\n",
        "\n",
        "def dg_dy(x, y):\n",
        "    return -5 * math.exp(-y)\n",
        "\n",
        "# Gradient Descent implementation\n",
        "def gradient_descent(start_x, start_y, learning_rate, iterations):\n",
        "    x = start_x  # Initialize x\n",
        "    y = start_y  # Initialize y\n",
        "\n",
        "    for i in range(iterations):\n",
        "        grad_x = dg_dx(x, y)  # Compute partial derivative with respect to x\n",
        "        grad_y = dg_dy(x, y)  # Compute partial derivative with respect to y\n",
        "\n",
        "        x -= learning_rate * grad_x  # Update x\n",
        "        y -= learning_rate * grad_y  # Update y\n",
        "\n",
        "        # Print progress for demonstration\n",
        "        print(f\"Iteration {i+1}: x = {x}, y = {y}, g(x, y) = {g(x, y)}\")\n",
        "\n",
        "    return x, y  # Return the values of x and y at the minimum\n",
        "\n",
        "# Parameters\n",
        "start_x = 1.0       # Initial guess for x\n",
        "start_y = 1.0       # Initial guess for y\n",
        "learning_rate = 0.01\n",
        "iterations = 10\n",
        "\n",
        "# Run Gradient Descent\n",
        "minimum_x, minimum_y = gradient_descent(start_x, start_y, learning_rate, iterations)\n",
        "\n",
        "# Print final result\n",
        "print(\"Value of x at minimum:\", minimum_x)\n",
        "print(\"Value of y at minimum:\", minimum_y)\n",
        "print(\"Minimum value of g(x, y):\", g(minimum_x, minimum_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xEBeHgcPGt7",
        "outputId": "15038d87-49b9-4851-8962-eb0fb9a5c8ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: x = 0.94, y = 1.0183939720585722, g(x, y) = 14.456672655087498\n",
            "Iteration 2: x = 0.8835999999999999, y = 1.036452698609447, g(x, y) = 14.115800473484644\n",
            "Iteration 3: x = 0.830584, y = 1.0541882345442934, g(x, y) = 13.811985306391312\n",
            "Iteration 4: x = 0.7807489599999999, y = 1.0716119941765265, g(x, y) = 13.540986991147753\n",
            "Iteration 5: x = 0.7339040224, y = 1.0887347959317717, g(x, y) = 13.299056069253297\n",
            "Iteration 6: x = 0.6898697810559999, y = 1.1055669032014577, g(x, y) = 13.082876799165636\n",
            "Iteration 7: x = 0.64847759419264, y = 1.1221180617486863, g(x, y) = 12.889516796660056\n",
            "Iteration 8: x = 0.6095689385410815, y = 1.1383975340101906, g(x, y) = 12.716382531363704\n",
            "Iteration 9: x = 0.5729948022286167, y = 1.1544141305988047, g(x, y) = 12.561179997918545\n",
            "Iteration 10: x = 0.5386151140948996, y = 1.1701762392765598, g(x, y) = 12.421879959594623\n",
            "Value of x at minimum: 0.5386151140948996\n",
            "Value of y at minimum: 1.1701762392765598\n",
            "Minimum value of g(x, y): 12.421879959594623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Define the sigmoid function z(x)\n",
        "def z(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "# Define the derivative of the sigmoid function dz/dx\n",
        "def dz_dx(x):\n",
        "    sigmoid = z(x)\n",
        "    return sigmoid * (1 - sigmoid)\n",
        "\n",
        "# Gradient Descent implementation\n",
        "def gradient_descent(start_x, learning_rate, iterations):\n",
        "    x = start_x  # Initialize x\n",
        "\n",
        "    for i in range(iterations):\n",
        "        grad = dz_dx(x)  # Compute the gradient\n",
        "        x -= learning_rate * grad  # Update x\n",
        "\n",
        "        # Print progress for demonstration\n",
        "        print(f\"Iteration {i+1}: x = {x}, z(x) = {z(x)}\")\n",
        "\n",
        "    return x  # Return the value of x at the minimum\n",
        "\n",
        "# Parameters\n",
        "start_x = 1.0       # Initial guess for x\n",
        "learning_rate = 0.1\n",
        "iterations = 10\n",
        "\n",
        "# Run Gradient Descent\n",
        "minimum_x = gradient_descent(start_x, learning_rate, iterations)\n",
        "\n",
        "# Print final result\n",
        "print(\"Value of x at minimum:\", minimum_x)\n",
        "print(\"Minimum value of z(x):\", z(minimum_x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfDbMKLtRmTP",
        "outputId": "63ddd7b1-a575-4d18-f5dc-a93315852152"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: x = 0.9803388066758518, z(x) = 0.7271754378182266\n",
            "Iteration 2: x = 0.9604996746306421, z(x) = 0.7232218371555797\n",
            "Iteration 3: x = 0.9404824734889533, z(x) = 0.7191971045098832\n",
            "Iteration 4: x = 0.920287210551505, z(x) = 0.7151006232159546\n",
            "Iteration 5: x = 0.8999140383622942, z(x) = 0.7109318371617088\n",
            "Iteration 6: x = 0.8793632623551356, z(x) = 0.7066902560354154\n",
            "Iteration 7: x = 0.8586353485491341, z(x) = 0.7023754607176331\n",
            "Iteration 8: x = 0.8377309312592016, z(x) = 0.6979871087982813\n",
            "Iteration 9: x = 0.8166508207842319, z(x) = 0.6935249401960842\n",
            "Iteration 10: x = 0.7953960110320217, z(x) = 0.6889887828542721\n",
            "Value of x at minimum: 0.7953960110320217\n",
            "Minimum value of z(x): 0.6889887828542721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Squared Error (SSE) function\n",
        "def squared_error(x_values, y_values, M, C):\n",
        "    total_error = 0\n",
        "    for i in range(len(x_values)):\n",
        "        predicted = M * x_values[i] + C\n",
        "        error = (y_values[i] - predicted) ** 2\n",
        "        total_error += error\n",
        "    return total_error\n",
        "\n",
        "# Gradient Descent implementation to minimize Squared Error\n",
        "def gradient_descent(x_values, y_values, start_M, start_C, learning_rate, iterations):\n",
        "    M = start_M  # Initialize M\n",
        "    C = start_C  # Initialize C\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # Calculate gradients\n",
        "        grad_M = 0\n",
        "        grad_C = 0\n",
        "        for j in range(len(x_values)):\n",
        "            predicted = M * x_values[j] + C\n",
        "            grad_M += -2 * x_values[j] * (y_values[j] - predicted)\n",
        "            grad_C += -2 * (y_values[j] - predicted)\n",
        "\n",
        "        # Update parameters\n",
        "        M -= learning_rate * grad_M\n",
        "        C -= learning_rate * grad_C\n",
        "\n",
        "        # Print progress\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i+1}: M = {M}, C = {C}, SSE = {squared_error(x_values, y_values, M, C)}\")\n",
        "\n",
        "    return M, C\n",
        "\n",
        "# Example data (x_values and expected y_values)\n",
        "x_values = [1, 2, 3, 4, 5]\n",
        "y_values = [3, 6, 9, 12, 15]  # Linear relationship: y = 3x\n",
        "\n",
        "# Parameters\n",
        "start_M = 0.0  # Initial guess for M\n",
        "start_C = 0.0  # Initial guess for C\n",
        "learning_rate = 0.01\n",
        "iterations = 100\n",
        "\n",
        "# Run Gradient Descent\n",
        "optimal_M, optimal_C = gradient_descent(x_values, y_values, start_M, start_C, learning_rate, iterations)\n",
        "\n",
        "# Print final result\n",
        "print(\"Optimal M:\", optimal_M)\n",
        "print(\"Optimal C:\", optimal_C)\n",
        "print(\"Final Squared Error:\", squared_error(x_values, y_values, optimal_M, optimal_C))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gdoAMkyRua3",
        "outputId": "286d50f2-ce42-4c52-d269-53466ef9dd80"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: M = 3.3000000000000003, C = 0.9, SSE = 17.100000000000005\n",
            "Optimal M: 2.9611416117409677\n",
            "Optimal C: 0.14029111131493294\n",
            "Final Squared Error: 0.017911973981824498\n"
          ]
        }
      ]
    }
  ]
}